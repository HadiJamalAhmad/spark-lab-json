[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Processing event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Users/hadij/Downloads/spark-lab-json/src/main/scala/com/tp/spark/core/Ex0WordcountDF.scala","languageId":"scala","version":1,"text":"package com.tp.spark.core\n\nimport org.apache.spark.{SparkConf, SparkContext, sql}\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.{DataFrame, SparkSession}\n\nobject Ex0WordcountDF {\n\n  val pathToFile = \"data/wordcount.txt\"\n\n  def loadData(): DataFrame = {\n    // create spark configuration and spark context: the Spark context is the entry point in Spark.\n    // It represents the connexion to Spark and it is the place where you can configure the common properties\n    // like the app name, the master url, memories allocation...\n    val conf = new SparkConf()\n                        .setAppName(\"Wordcount\")\n                        .setMaster(\"local[*]\") // here local mode. And * means you will use as much as you have cores.\n\n    val ss = SparkSession.builder()\n      .config(conf)\n      .getOrCreate()\n\n    ss.read.csv(pathToFile).toDF(\"line\")\n  }\n\n  /**\n   *  Now count how much each word appears!\n   */\n  def wordcountDF() : DataFrame = {\n    ???\n  }\n\n  /**\n   *  Now keep the word which appear strictly more than 4 times!\n   */\n  def filterOnWordcountDF() : DataFrame = {\n    ???\n  }\n\n}\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0manalysis location (C:\Users\hadij\Downloads\spark-lab-json\target\scala-2.12\zinc\inc_compile_2.12.zip,true)[0m
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 1 s, completed 24 fÃ©vr. 2022, 08:06:19[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Done event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0mForcing garbage collection...[0m
